---
title: "DQN을 이용한 자율주행"
excerpt: ""

categories:
  - 머신러닝
tags:
  - 머신러닝
  - 자율주행
---
### DQN을 이용한 자율주행
- 현실에서 학습을 시키는 데에서의 어려움
  - 장비없이 하나의 에피소드가 끝났다는 사실을 DQN에게 알리는 것이 어려움
  - 차량에 충돌 감지 장치가 없다면 위 그림처럼 충돌 후에도 계속 주행 데이터가 쌓일 가능성이 있음
  - 에피소드가 끝날 때마다 사람이 일일히 출발 위치에 가져다 놓기가 어려움
  - 모든 것을 자동으로 알아서 해줄 편한 무언가 필요함
  
- 시뮬레이터 사용
  - XYCAR SIMULATOR를 이용하여 그 안에서 학습시킨 후 자율 주행을 시도할 예정
    - 시뮬레이터 사용시 충돌 순간을 감지하여 현재 에피소스를 중지 시킬 수 있음
    - 사람이 일일히 하지 않아도 자동으로 차량위치 센서의 상태 등을 초기화 가능  
  - 다만 시뮬레이터와 현실은 엄연한 차이가 있으므로 우리는 시뮬레이터를 현실에 맞도록 수정 작업을 거쳐아 함
  
### Xycar와 Lidar
### Pixel & Meter 단위 환산
- 시뮬레이터와 현실과의 간극 메우기
  - 시뮬레이터 환경과 현실과 결정적으로 다른 부분은 바로 '단위'
  - 보통 이런 경우 고정되어 있는 사이즈를 기준으로 시뮬레이터의 단위와 현실 단위 사이의 비율을 계산하고,
  - 시뮬레이터에서 학습된 데이터를 현실에 적용할 때 해당 비율을 사용하는 방법으로 간격 메우기
  
- pixel m 단위
  - 차량 규격
    - 가로: 약 0.3m
    - 세로: 약 0.6m
  
  - 시뮬레이터 차량 규격
    - 가로: 64px
    - 세로: 128px
    - 가로:세로 = 1:2
  
  - px과 m 비율
    - 차량의 크기는 변하지 않기 때문에 이를 기준으로 비율 측정
    - (차랑 길이 m):(이미지 상 차량 길이 px) = (구하고자 하는 길이 m):(구하는 대상이 되는 이미지 길이 px)
    - 0.6m:128px = x m:y px
  
  - 시뮬레이터 규격
    - width: 1000px
    - height: 720px
    - road_width: 220px
  
  - m로 환산
    - width: 약 4.69m
    - height: 약 3.37m
    - road_width: 1.03m
  
- pixel와 m 단위 환산 - ratio, velocity
  - 시뮬레이터에서는 px 단위로, 실제로는 m 단위로 모든 것을 하기 때문에 velocity도 바꿔야 함
  - Velocity
    - 시뮬레이터의 100px/s를 m/s로 변환
    - 100px/s는 0.469m/s로 바꿈  
  - Ratio
    - px와 m비율: 213.33
  
- 시뮬레이터와 실제 map 규격
  - 시뮬레이터
    - width: 1000px
    - height: 720px
    - road_width: 220px
    - car: 128px  
  - 실제 map
    - width: 4.69m
    - height: 3.37m
    - road_width: 1.03m
    - car: 0.6m
  
### 환경구축
- 작업환경 구축 (파이썬 패키지 설치)
  - Pytorch v1.2
  - Pyglet v1.4.11
  - Visdom v0.1.8.9
  - Pygame v1.9.6
  - Dill v0.3.3

### Xycar 시뮬레이터를 사용한 DQN 기반 자율주행
- 목표
  - 거리센서 정보를 사용할 수 있는 시뮬레이터 에서
  - 강화학습 DQN 기밥으로 학습을 시켜서
  - 자동차가 벽과 충돌하지 않고 주행하도록 만들기
  
- 작업 환경
  - ~/xycar_ws/src/DQN
  
- 작업 내용
  1) main.py 파일의 빈 부분을 xycarRL 패키지를 이용하여 채워넣고
  2) main.py 파일에 있는
    - 하이퍼 파라미터(hyper_param) 값을 바꾸고
    - 스테이트(state_select) 값을 바꾸고  
  3) my_reward.py 파일에 보상정책에 관한 코드를 작성하여 DQN 모델의 학습 진행하기
  
  - 학습 과정
    - 자동차가 최대한 빠르게 한바퀴를 주행하도록 만드는 것이 목표
    - 학습을 반복하면서 이전 Score보다 이전 높은 Score가 나오면 그 때의 weight 파일이 .pth 파일로 ~/DQN/save/ 에 저장됨
    - 또한 주행 모습을 담은 .avi 파일이 만들어져 ~/DQN/video/ 에 저장됨
    - 저장된 .pth 파일은 viewer.py 파일을 사용하여 살펴 볼 수 있음
  
- 필요한 파일 다운로드 & 설치
  - DQN.tgz
  - DQN/env/
  
- 3개 파일 생성
  - main.py
  - my_reward.py
  
- main.py 생성
  - 중요한 기본 코드가 담겨 있는 파일
    - main.py파일에서 주석 표시된 부분을 채워 넣어야 함
    - 채워 넣을 함수는 제공한 xycarRL.py 파일안에 있음
- 학습 상황 살펴보기
  - Visdom 시각화 도구 사용
    - Reward 그래프
    - Dead Position
    - Loss 그래프
    - Learing Curve 그래프
  
- my_reward.py 생성
  - 강화학습은 보상을 언제 어떻게 얼마나 주느냐에 따라서 다른 형태를 보이는데 이 파일로 보상 정책을 세울 수 있음
  - for_reward 리스트안에 필요한 데이터를 넣어 온 후 원하는대로 리워드를 설계
  - 코너에서 붙어서가면 보상을 더주는 방식으로 설계

- my_reward.py 소스코드 예시
  - reward는 환경정보를 쓰는데 agent가 모르는 정보를 쓰면 안됨(존재하는 정보로만으로 reward를 만들어야 함)
  
- viewer.py 사용법
  - pth 파일의 시험 주행을 위한 프로그램
  - pth 파일은 학습된 데이터의 저장파일 확장자
  - Viewer.py를 아래와 같이 살짝 수정하는 것으로 학습한 pth 파일을 테스트해볼 수 있음
    - hidden_layer
      - 학습시에 사용한 main.py >> hidden_size 파라미터와 똑같이 입력  
    - lidar_cnt
      - 학습시에 사용한 센서의 개수 입력  
    - state_select = _
      - 학습시 사용한 데이터의 종류를 체크
      - 가능한 데이터 종류
        - car sensor: 거리센서
        - car yaw: 자동차 방향
        - car position: 자동차 좌표
        - car steer: 자동차 조향값  
    - xycar.ML_init("DQN")
      - 학습시에 사용한 모델을 종류
      - 가입 가능한 종류로는 "DQN", "DDQN", "Duel DQN"  
    - view_epi = 404
      - 저장된 학습파일의 에피소드 넘버 입력(파일이름 끝에 적혀있는 숫자)
  
### XycarRL Package
- xycarRL Package
  - xycarRL.py 파일은 main.py 에서 사용하는 라이브러리
  - xycarRL.so 파일(패키지)에는 환경 초기화, 환경 정보 수신, 학습 등 시뮬레이터 환경에서 강화학습을 구동하기 위해 필요한 모든 것이 들어있음
  
- xycarRL 함수 설명
  - xycar.set_map(string)
    - 반환 값: void
    - 맵을 설정하는 함수
    - "square", "snake"  
  - xycar.get_max_score()
    - 반환 값: float
    - 지금까지 최고 점수를 리턴하는 함수
    - 자동갱신이 되지 않으므로 max_score_update와 함께 사용하는 것이 좋음  
  - xycar.max_score_update(float)
    - 반환 값: void
    - Max score를 저장하는 변수에 입력된 파라미터 값을 저장함
  - xycar.set_frame(float)
    - 반환 값: void
    - viewer.py 사용시 Fps = (Frame/sec)를 설정하는 함수  
  - xycar.set_hyperparam(dict)
    - 반환 값: void
    - 하이퍼 파라미터를 설정하는 함수
    - 입력되는 딕셔너리의 내용
      - sensor_num, Learning_rate, Discount_factor, Optimizer_step, hidden_size
      - batch_size, min_history, buffer_limit, max_episode, update_cycle  
  - xycar.state_setup(dict)
    - 반환 값: void
    - Input size를 결정하는 변수
    - 해당 변수의 키 값
      - car seosor, car yaw, car position, car steer  
  - xycar.ML_init(string)
    - 반환 값: void
    - 학습시 저장할 모델 설정 가능
    - 지정 가능한 모델은 "DQN", "DDQN", "Duel DQN"  
  - xycar.Experience_replay_init()
    - 반환 값: void
    - Experience_replay 버퍼를 초기화 하는 함수  
  - xycar.Experience_replay_memory_input(np.ndarray, int, np.ndarray, float)
    - 반환 값: None  
    - Experience_replay memory에 state, action, next state, reward 값 저장  
  - xycar.Expereince_replay_close()
    - 반환 값: void
    - Experience_replay 버퍼를 닫는 함수  
  - xycar.pygame_exit_chk()
    - 반환 값: bool
    - Pygame이 종료되었는지 확인하는 함수
    - 종료되었다면 True 반환  
  - xycar.set_E_greedy_func(float)
    - 반환 값: void
    - Egreedy 함수 추가 모험의 수행을 결정하는 기준 숫자를 설정하는 함수  
  - xycar.get_action(np.ndarray)
    - 반환 값: int
    - 상태값을 입력하여 어떤 행동을 취할지 결정하는 함수  
  - xycar.step(int)
    - 반환 값: np.ndarray
    - 자동차 환경을 한 스텝만큼 진행시키는 함수
    - 입력된 action 값으로 인해 변화된 상태값을 반환  
  - xycar.set_hidden_size(list)
    - 반환 값: void
    - 학습시 사용할 네트워크의 히든 레이어를 설정하는 함수  
  - xycar.train(int)
    - 반환 값: void
    - Experience Replay에 쌓여 있는 데이터를 바탕으로 학습하는 함수
    - 입력값은 optimizer의 매개변수를 갱신할 횟수
    - 자세한 설명은 hyper parameter의 optimizer step 참고  
  - xycar.mainQ2targetQ()
    - 반환 값: void
    - 지금까지 학습된 Target-Q를 Main-Q로 갱신하는 함수  
  - xycar.set_init_location_pose_random(bool)
    - 반환 값: void
    - 차량의 초기 위치를 랜덤하게 시작하도록 할 것인지, 고정된 위치에서 시잘할 것인지 결정
    - True 일시 차량의 초기 위치가 랜덤  
  - xycar.get_episode_total_time()
    - 반환값: float
    - Episode 시작부터 이 함수를 호출할 때까지의 시간을 반환하는 함수  
  - xycar.get_sensor_value()
    - 반환 값: list
    - 관측된 라이다 센서의 값을 반환하는 함수
    - 우측 그림과 같이 [0, 1, 2, 3, 4] 형식으로 저장되어 있음  
  - xycar.get_xycar_position()
    - 반환 값: list
    - 자동차의 현재 위치를 반환하는 함수
    - [x좌표, y좌표] 형식으로 저장되어 있음
  - xycar.get_xycar_yaw()
    - 반환값: float
    - 자동차의 현재 방향을 반환하는 함수  
  - xycar.get_xycar_steering()
    - 반환값: float
    - 자동차의 현재 조향값을 반환하는 함수  
  - xycar.get_step_number()
    - 반환값: int
    - 진행중인 episode에서 현재 처리중인 step의 번호를 반환하는 함수  
  - xycar.get_episode_done()
    - 반환 값: bool
    - 현재 에피소드의 종료 여부를 반환하는 함수  
  - xycar.get_round_count()
    - 반환 값: int
    - 자동차가 몇 바퀴 돌았는지 반환하는 함수  
  - xycar.get_memory_size()
    - 반환 값: int
    - Experience_replay에 저장되어 있는 데이터의 크기를 반환하는 함수  
  - xycar_set_lidar_cnt(int)
    - 반환 값: void
    - 거리 데이터를 몇 개 사용할 것인지 결정하는 함수
    - 자동으로 개수를 선택하면 정면 180도 안에서 일정한 간격으로 입력한 파라미터 값만큼 데이터를 뽑아줌  
  
### DQN2Xycar 패키지 제작
- ROS 패키지 설계
  - DQN을 받아서 A2로 실행
  - viewer.py 파일을 변경해서 패키지 만들 계획
  - RosModule.py에서 라이다 노드에서 라이다 데이터를 받아서 모터 노드에 모터 데이터를 제공
  
### DQN 학습 Weight 적용
- 학습 weight을 차량에 적용
  - 적용할 weight 파일을 dqn2xycar/src/save 폴더에 옮기기
  - 파이썬 코드(viewer.py) 상의 view_epi를 weight 파일 번호로 변경
  - 파이썬 코드 hidden_layer를 학습 시 사용한 노드 개수를 입력
  - 파이썬 코드 lidar_cnt를 학습 시 사용한 거리측정장치 개수 입력
  
- 실패 원인 가정
  - 앞서 만든 weight 파일 차에 적용시켜 주행 -> 실패
  - Why
    - 라이다가 측정할 수 있는 거리는 최대 값이 존재함
    - 그래서 최대한 값을 넘어가는 거리일 때 inf 값
    - 또한 거리가 너무 가까울 때도 거리를 측정하지 못해 inf 값
    - 하드웨어 특성으로 좋지 않은 환경에 인식하지 못함
  
  - 그렇다면 학습할 때도 적정량의 inf 값을 주면 제대로 학습되지 않을까?
  
- inf 값을 줘서 시뮬레이터 학습
  - 주행 중 inf 값이 어니 증도 나오는지 측정
    - 실제 라이다의 1 frame에 들어오는 라이더 데이터 중 inf의 비율을 측정
    - 측정 시 180개의 데이터 중 30~40개 정도 inf 값이 들어옴을 확인  
  - 시뮬레이터에서 라이다의 데이터를 30~40개 정도 랜덤으로 선택하여 inf 값으로 변경하여 학습 => 실패
  - Why?
    - inf 값은 무한대의 수, Loss 계산 시 터무니 없는 값이 나오는 경우가 생기기 때문에 학습하기 곤란
  
- inf를 라이다의 최대 값을 변경
  - 그렇다면 inf를 유한하게 만들면 어떨까?
    - 라이더의 최대 측정 거리는 14m, 이를 px 단위로 변경(14m = 약 2986.62px)  
  - 시뮬레이터에서 라이다의 데이터를 30~40개 정도 랜덤으로 선택하여 2986px 값으로 변형하여 학습 => 실패
  - why?
    - inf 값과 같이 Loss 값이 터무니 없는 숫자가 출력
    - 그러나 현실에서는 너무 멀거나 너무 가까울 때도 inf 값이 나옴
    - 그리고 주변 환경의 영향(빛 반사, 습도 등)으로 inf 값이 나오기도 함
    - 따라서 학습하기 곤란
  
- inf 주변 값 평균
  - 지금까지는 학습 시 시뮬레이터를 현실에 맞추려고 했지만, 역발상으로 현실의 데이터를 시뮬레이터스럽게 바꿔서 학습하는 방법을 사용
  - 시뮬레이터에서는 무한대 값이 존재하지 않으며, 튀는 값 없는 거리 데이터 구조를 유지
  - 그래서 우리는 앞으로 라이다 데이터를 수신했을 시, 차이의 입장에서는 장애물이 가까울수록 굉장히 불리해지므로, 해당 각도의 좌 우의 거리값을 비교하여 가장 작은 값을 inf 대신 넣는 방법을 사용
  
- inf 주변 값 중 최소값
  - 라이다의 값
    - 360도의 거리가 배열의 형태(인덱스 0~259)로 들어옴
    - 우리가 사용할 범위는 0~180 -> 뒤쪽은 차량의 방해를 받고 필요가 없음  
  - 인덱스가 0일 때,
    - 0보다 앞선 인덱스(-1)가 없기 땜ㄴ에 인덱스 0보다 큰 값들을 사용
    - 인덱스 1의 값과 2의 값으로 최소값을 구함  
  - 인덱스가 180일 때,
    - 인덱스 180의 뒤에 값(181)이 없기 때문에 인덱스 180보다 작은 값들을 사용
    - 인덱스 179의 값과 178의 값으로 최소값을 구함
  
  - current_ipt[i] == float('inf')
    - if idx[i] == 0:
      - 인덱스가 0인 laser_msg의 경우엔 자기 자신과 1과 2를 이용하여 최소값을 구함  
    - elif idx[i] == 179:
      - 인덱스가 179인 laser_msg의 경우엔 178과 177을 이용하여 최소값 구함
    - else:
      - 인덱스가 0과 179가 아닌 경우엔 양 옆의 수와 비교하여 3개중 최소값을 구함
  
  - 인덱스 359, 181을 사용하지 않는 이유
    - inf가 반복될 때 차량의 방해를 받는 값을 가져올 수 있고, 조금 더 간결한 수식을 위해
  
  - inf를 주변 값 중 가장 작은 값을 줘서 주행 => 성공
  - Problem
    - 제대로 주행은 하지만 코너를 돌 때 코너와 근접하게 붙어서 주행
    - 따라서 차량이 코너에 걸리는 상황이 발생
    - 차가 중심을 맞춰서 주행할 수 있도록 학습
  
- 중심을 맞춰서 주행  
  - 중심을 맞추기 위해서는 어떻게 학습 리워드 알고리즘을 짜야할까?
    - 라이다 값으로 계산된 방향과 지금 주행하고 있는 방향을 비교해서 그 차이를 0과 1 사이의 값으로 환산해서 reward를 주자  
  - 고려해야 할 부분
    - 기계는 사람이 아니므로 리워드 함수가 학습 중에 계속 바뀌는 값을 도출해내면 학습할 때 에이전트 입장에서 굉장히 혼라스러울 수 있음
    - 따라서, function(input value) = reward 형식으로 짜야 함
  
  - 구해야 하는 값: 현재 방향을 나타내는 각도 & 라이다 값으로 계산된 각도
    - 방향은 벡터의 값이므로 라이다 값들을 사용하여 벡터(방향)를 계산해야 함
    - 현재 바퀴의 각도 => 미래에 가야할 곳, 현재 방향: yaw 값
    - 라이다의 값들을 사용해 벡터의 평균을 구해 나온 각도 => 적절한 방향(미래의 길)

- 중심을 맞춰서 주행 - 현재 차량(가야할)의 방향
  - 구해야 하는 값 중 현재 방향을 가져오기
  - 현재 방향은 차량의 조향 값(세타)를 가져와야 함
  - 차량 조향 값의 최대(30), 최소(30)
  
- 중심을 맞춰서 주행 - 라이다 값의 방향
  - 조향값은 x, y 성분 전체를 가져와서 평균
  - 구해야 하는 값 중 라이다 값의 방향 가져오기
  - x 성분: sin세타 * 거리
  - y 성분: cos세타 * 거리
  - sin세타 = x 값 / 거리
  - cos세타 = y 값 / 거리
  - x 값 = sin세타 * 거리 = x 성분
  - y 값 = cos세타 * 거리 = y 성분
  
  - 바퀴 조향
    - 직진: 0
    - 오른쪽: 음수
    - 왼쪽: 양수
  
  - 라이더 값 순서
    - 오른쪽: 인덱스 0
    - 왼쪽: 인덱스 180
  
  - 각도의 순서
    - 반시계 방향
   
  - 각도를 나타내는 단위: radians & degrees
    - Radians
      - 1radians: 원주의 호의 길이와 반지름의 길이가 같을 때의 각도
      - 원의 한바퀴는 2파이  
    - Degrees
      - 원의 한바퀴를 360도로 표현한 단위
    - 파이썬 math 모듈은 radians 단위로 계산하기 때문에 degrees로 표현된 각도 계산 시 radians 단위로 변경해줘야 함
  
  - x 성분과 y 성분
    - 라이다 값은 5개를 사용하기 때문에 각 성분 5개의 평균을 구해서 사용
  
  - 평균 벡터 거리
    - 위에서 구한 x 성분의 평균과 y 성분의 평균을 사용해서 구해지는 거리
    - 평균 벡터 거리 = 루트(x 성분 평균 제곱 더하기 y 성분 평균 제곱)
  
  - 지금까지 구해진 값
    - x 성분 평균
    - y 성분 평균
    - 평균 벡터 거리
  
  - 각도 세타를 구하는 방법
    - arcsin(x 성분 평균 / 평균 벡터 거리)
    - arccos(y 성분 평균 / 평균 벡터 거리)
    - 우리는 좌측은 양수, 우측은 음수인 값을 도출해야 하므로 위 공식 중 arcsin 사용해야 함
      - sin: 좌측 - 양수, 우측 - 음수
      - cos: 좌측 - 양수, 우측 - 양수  
    - 구해진 세타는 Radians 단위이고, 차량의 방향읜 Degrees단위를 사용하므로 변환해줌
    - 구해진 각도가 -30 ~ 30의 범위를 벗어나면 차량의 방향의 최대, 최소 값으로 세팅해줌
  
- 중심을 맞춰서 주행 (두 값 비교하기)
  - 두 값의 차
    - 현재 차량의 방향 =- 라이다 값으로 구해진 방향  
    - 리워드는 0에서 1사이의 값으로 나워야 하므로, 두 값의 차를 0~1 사이의 값으로 변환해야 함

  - 0과 1 사이 값으로 변환
    - 두 값의 최대, 최소 값은 30과 -30이므로 총 60
    - 따라서 두 값의 차는 -60부터 60까지의 값이 나옴
    - 두 값의 차를 절대 값으로 변환하면 0부터 60까지의 값이 나옴
    - 절대 값으로 변환한 두 값의 차를 60으로 나누면 0과 1사이의 값이 나옴
    - Reward = |라이다 벡터 평균의 사이각 - 스티어링 값| / 60
  
- 중심으로 맞춰서 주행(과제 - 코드설명 있음)
  - reward_in_game(data1, data2)
    - 주행 진행 시 주는 리워드 함수
    - data1: 라이다 값
    - data2: 조향 값(steering)
    - reward_cal: 중심 맞춰 주행하는 것에 대한 리워드를 계산 함수  
  - reward_end_game(data)
    - 주행 종류 시 주는 리워드 함수
  
- 중심을 맞춰서 주행 (state에 steering 추가)
  - state에 steering 추가
    - steering 값을 reward 계싼에 사용하기 때문에 이를 state에 추가해줘야 함
    - state에 없는 값을 넣어 reward를 부여하면 그 값이 state가 아닌 외부 요인으로 작용하여 노이즈와 같이 작용됨
    - 따라서 reward 계싼에 필요한 값은 꼭 state에 추가해야 함
  
  - Visdom
    - reward value: 특정 에피소드(x축)에서 몇 score를 획득했는가
    - depth position: 차가 죽었을 때 위치(x, y 좌표)
    - Learning Curve: 학습이 얼마나 잘되고 있는지 판단하는 척도(부드럽다가 점차 올라가는 것이 이상적)
    - Loss Graph: 점점 줄어드는 그래프가 이상적
  
- 앞에서 만들어진 reward 함수로 학습 진행 => 제대로 안됨
  - 주행하는 모습을 보면 우측 화살표와 같이 처음엔 우회전으로 잘 돌지만 두번째 좌회전을 해야 할 때도 계속 우회전 방향으로 주행함
  - why?
    - 학습 패턴을 보면 E-greedy 함수대로 초반부 코너의 우회전은 학습이 잘 됨.
    - 후반부의 좌회전부터는 모험을 거의 안해서 학습이 잘 안되는 것으로 보임
  
- 랜덤한 주행 시작 위치
  - 후반부 학습이 잘 안되는 문제 해결 방법
    - 그렇다면 주행 시작 위치를 랜덤하게 주면 여러 방향으로 학습할 수 있지 않을까?
    - 시작 위치가 바뀌면서 다양한 커브를 학습해 균형적인 학습이 가능해짐  
  - set_init_location_pose_random(Bool) 함수를 참조
    - 해당 함수 사용시 인자를 False를 넣으면 특정 위치에서만 시작하지만 True를 넣으면 랜덤으로 위치가 바뀌어서 시작
  
- DDQN 학습 그래프
  - DDQN을 사용해서 학습
  
- DDQN 학습 weight 선택
  - 학습 후 나온 video를 보면서 적절한 학습 weight를 선택
  - 우리의 목적은 중심을 맞추려고 노력함으로써 벽에 닿지 않고 주행하는 것
  - 해당 시뮬레이터 사용시 의도대로 조금씩 띄우려고 하다가 어느 순간부터 다시 붙어가는 것을 확인할 수 있음
  - 우리가 필요한 것은 살짝 떨어진 것으로 너무 많은 학습은 필요하지 않다는 것을 알 수 있음
  - 따라서 2000번대 학습 파일을 사용하면 적당할 것으로 보임
  
- DQN과 DDQN
  - DQN의 문제
    - Loss 그래프가 잘 안 줄어들음
    - 일반적인 딥러닝에서는 줄어들어야 하지만, DQN에서는 잘 줄어들지 않음
    - 이러한 DQN의 문제를 해결한 것이 DDQN
  
  - DQN과 DDQN 코드(xycarRL.py)
    - DQN
      - max_q_prime = self.q_target(next_state).max(1)[0].unsqueeze(1)  
    - DDQN
      - max_q_prime = self.q_target(next_state).detach().gather(1, self,q(next_state).detach().max(1)[1].unsqueeze(1))
  