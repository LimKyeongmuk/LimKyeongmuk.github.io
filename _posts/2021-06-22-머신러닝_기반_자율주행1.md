---
title: "머신러닝 기반 자율주행1"
excerpt: ""

categories:
  - 머신러닝
tags:
  - 머신러닝
  - 자율주행
---
### 머신러닝 기반 자율주행 SW 만들기
- 미로 주행
  - 시뮬레이터
    - 5개 거리센서
    - 속도/조향각 조정
  
  - 2가지 구현 방법
    - 알고리즘 기반 자율주행 SW
    - 머신러닝 기반 자율주행 SW
  
- 머신러닝을 이용한 자율주행 SW 만들기
  - 운전대를 인공지능에게 넘기기
  - 거리 센서 데이터 5개를 알려주기
  - 핸들을 어떻게 꺾어야 할지 결정하기
  
- 어떤 머신러닝 기술 필요
  - 강화학습
    - 반복적인 시행착오를 거쳐 가장 가성비 좋은 운전 요령을 터득하는 학습 방식  
  - Q-Learing
    - 강화 학습 기법 중 하나
    - 모델 없이 학습하는 강화학습 알고리즘(요령을 적어두는 표를 만들기)
    - 경우의 수가 많은 복잡한 문제는 풀 수 없음  
  - DQN(Deep Q Network)
    - 구글 딥마인드에서 발표한 논문에서 소개한 기법
    - 인공 신경망에 기반한 학습모델을 사용하는 강화학습 방법

### 강화학습 개요
- 강화학습(Reinforcement Learing)
  - 기계학습(ML)의 종류에는 지도학습, 비지도학습, 강화학습이 존재
  - 시행착오 끝에 보상을 많이 받을 수 있는 요령을 터득
  - 에이전트가 환경에 대해서 action을 취하면 state가 바뀌고 그에 따른 보상이 주어짐
  - 보상값을 관찰해서 판단한 후, 정책(policy)를 세워서 다른 action을 취하기 반복
  
- 강화학습 사례
  - 강화학습을 통해 주차 요령 터득하기
    - https://www.youtube.com/watch?v=VMp6pq6 Qjl
  
- 강화학습의 특징
  - 아무것도 가르쳐 주지 않음
  - 일단 시도 > 계속 반복 > 수 많은 시행착오 끝에 성공
  - 시행착오
    - 이렇게 하는 게 아니였구나 (패널티)
    - 저번 게 더 좋았구나 (보상값)
  
- 강화학습의 개넘
  - 학습 과정
    1. 정의된 주체(agent)가 주어진 환경(enviroment)의 현재 상태(state)를 관찰(observation)하여 이를 기반으로 행동(action)을 취함
    2. 이때 환경의 상태가 변화하면서 정의된 주체는 보상(reward)을 받게 됨
    3. 이 보상을 기반으로 정의된 주체는 더 많은 보상을 얻을 수 있는 방향(best action)으로 행동을 학습하게 됨
  
  - 이용(exploitation)과 탐험(exploration) 사이의 균형
    - 이용(exploitation): 현재까지의 경험 중 현 상태에서 최대의 보상을 얻을 수 있는 행동을 수행하는 것
    - 탐험(exploration): 다양한 경험을 쌓기 위해서 새로운 시도를 수행하는 것
  
  - 마르코프결정 프로세스(MDP)
  
- 강화학습 기법들
  - Q-Learing
  - SARSA
    - State-action-reward-state-action  
  - DQN
    - Deep Q Network
    - 구글 딥마인드에서 2014년 내놓은 논문  
  - DDPG
    - Deep Deterministic Policy Gradient
    - 심층 결정론적 정책 경사법 알고리즘  
  - A2C
    - Advantage Actor-Critic
  
### Q-Learning 기법
- Q-Learning 개념
  - Q-Learning(Q table)
    - 보상을 예측하는 함수인 Q 함수를 학습하는 것
    - Q 함수를 통해 더 큰 보상을 얻을 수 있는 action을 취할 수 있음
    - 모델 없이 학습할 수 있음(table에 값을 채우는 방식)
  
- OpenAI GYM 소개
  - OpenAI GYM: 인공지능 개발을 위한 여러 환경을 제공하는 프레임워크
  - ex) Frozen Lake
  
- OpenAI GYM 활용
  - 실습환경 구축
  - Python + TensorFlow / Pytorch + OpenAI GYM
  
- 예제 코드 (Frozen Lake)

- Q-function(state-action value function)
  - 해당 상태에서 각 행동으로 얻을 수 있는 보상의 Quality를 수치로 알려줌
    - 최대 보상을 얻을 수 있는 행동을 선택할 때 사용
    - Q가 크다는 것은 해당 행동을 취했을 때 얻을 수 있는 보상이 크다는 의미
  
  - 상태 s에서 Q가 가지는 최대값
  - 상태 s에서 Q가 최대값을 가지게하는 argument a
  - *: optional
  - 즉, Q가 큰 actiono을 취해야 큰 reward를 얻을 수 있음
  
- Q-function learning 방법
  - 변화된 상태 s'에서 Q가 있다고 가정, Q(s', a') 존재
  - 상태 s에서 행동 a를 취했을 때 상태는 s'가 되고 보상 r을 받음
  - Q(s, a): 상태 s에서 행동 a를 취할 때의 Q
  - ^: 예측값
  
  - 지금 상태 s에서 a라는 행동을 취했을 때 얻게되는 보상값은 a라는 행동을 통해 얻는 다음 단계 상태인 s'에서 앞으로 얻을 것으로 예상되는 미래의 보상값의 최대치와 이번에 얻게되는 보상값 r을 더한값이 됨
  - 미래의 예상되는 보상값은 일단 추측값
  
- Q-function learing 과정
  - Q(s, a): 16x4 table
  - 16 states and 4 actions
  - Q를 update
    - 항상 goal에 도착하지만 비효율적인 길로만 갈 수 있기 때문에 다른 action 찾아가기
  
- Exploitation vs Exploration
  - decaying E-greedy
    - 특정 확률(e)로 random한 action을 취함
    - 학습이 진행될수록 e가 작아지므로 random한 action을 취할 확률이 낮아짐
  
  - add random noise
    - Q 값에 random한 value(noise)를 더함
    - noise가 더해진 Q 값 중 가장 큰 Q값을 가지는 action을 선택
    - 학습이 진행될수록 noise는 줄어드므로 영향이 적어짐
  
  - 문제 발생
    - 더 좋은 길을 구별하지 못함
  
  - Discounted reward
    - 미래에 얻어지는 보상은 작게 더해지도록 가중치를 줌
  
- Stochastic(non-deterministic, 비결정적/확률론적)
  - action에 따라 state가 보장되지 않을 수도 있음
  - right key를 눌러도 실제 action은 right가 아니라 down일 수 있음
  
- Stochastic(확률론적) 특성의 해결방안
  - Q가 학습될 때 learing rate를 통해 조금씩만 바뀌도록 함(천천히 학습)
  - 수식
  
- Q-Learning 알고리즘 동작 순서
  - Q(s, a)를 0으로 초기화
  - 현재 상태 s를 가져옴
  - 학습 반복
    - action을 결정하고 실행
    - 새로운 상태 s'과 보상 r을 받음
    - Q(s, a)를 업데이트
      - 수식  
    - 상태 업데이트
      - 수식
  
- 결과
  - Q-table을 얻게 됨
  
### DQN 기법
- Q-Learning의 한계
  - 선택할 수 있는 Action의 종류가 많은 경우
  - State 상태가 매우 많은 경우
  
  - 차원의 저주 & 계산의 복잡도
    - 데이터의 차원이 증가할수록 해당 공간의 부피가 기하급수적으로 늘어남
    - 차원이 증가할수록 모델 추정에 필요한 데이터 개수가 기하급수적으로 늘어남
  
  - Q-Learing은 근본적으로 상태가 적은 문제에만 적용 가능
  
- Q Network
  - Q Learing의 문제를 해결하기 위해 인공신경망을 도입
  
- DQN(Deep Q Network)
  - Q Network와 인공신경망의 성공적인 융합 사례
  - AlpaGo를 개발한 DeepMind의 논문에서 처음 소개
    - "playing Atari with Deep Reinforcement Learing"  
  - DQN은 중요한 혁신 2가지 존재
    - Experience Replay 기법 사용
    - 'Target Network'와 'Update Network' 분리
  
- DQN 이전의 문제
  - Correlations between samples
    - 가져오는 state 들이 서로 유사함
    - linear regression이 어려움
  
  - Non-stationary targets
    - loss를 구하는 cost function 식에서 세타(weight)가 공통적으로 포함되어 있음
    - 학습 대상과 target(label)이 같은 네트워크를 사용함
    - 세타를 학습(update)하면 target의 세타도 변하여 target이 움직임
  
- DQN의 solution
  - Go Deep
    - layer를 늘려서 network를 더 깊게 만듦  
  - Capture and replay
    - Correlation between samples 문제를 해결
    - action 후 결과들로 바로 학습하는 것이 아니라 저장했다가 나중에 학습  
  - Separate networks
    - Non-stationary targets 문제를 해결
    - 'Target Network'와 'Update Network'를 분리(목표가 움직이지 않도록)
  
- Solution 1) Go Deep
  - 더 깊은 네트워크를 사용하여 성능 개선
  
- Solution 2) experience replay
  - action을 취한 후 결과들로 바로 학습을 하지 않고 버퍼에 저장
  - 시간이 지난 후 버퍼에서 random하게 minibatch를 만들어 꺼내서 학습
  - random하게 가져온 data들은 서로 비슷하지 않음
    - linear regression 하기에 유리함  
  - Agent가 환경에서 탐험하며 얻은 샘플을 모두 메모리에 저장
  - Replay 메모리에서 무작위로 뽑은 샘플에 대해 인공 신경망을 업데이트
  - Non Esperience Replay 경우
    - 훈련중 안좋은 상황이 지속될 경우 계속 안좋은 방향으로 학습이 진행됨
    - 정작 좋은 상황에 대해서는 학습이 전혀 안될 수 있음  
  - Esperience Replay 경우
    - 학습 샘플의 시간적 상관관계가 없으므로 좋은 상황과 나쁜 상황이 섞여서 학습
    - 샘플 데이터를 여러 번 재사용할 수도 있음
  
- Solution 3) non-stationary targets
  - 네트워크를 분리함(두 개의 네트워크를 만들음)
    - Q(Ws)를 예측하는 네트워크, 세타 학습
    - Y(target, label)을 가져오는 네트워크, 바세타 유지  
  - 일정 episode 후 아래의 바세타를 위의 세타로 update
  
- Deep Q-Learning 알고리즘
  - Replay memory라는 버퍼를 생성
  - 두 개의 네트워크(DQN)를 생성
  - For episode
    - 처음의 상태 s1를 가져오고
    - while not done
      - E-greedy 방법을 통해 action 결정
      - action을 취하고 나온 결과들을 버퍼에 저장  
    - 버퍼에서 random하게 mini batch 만들기
      - target DQN(label)과 main DQN(예측 Q)를 비교하여 main DQN의 세타만 학습
      - 주기적으로 main DQN을 target DQN에 대입(update)
  
- DQN 흐름(Flow)
  - 자료 보기
  
- 초기화
  - 환경 초기화
    - 학습 시작 전 환경 초기화
    - 초기화 함수의 출력값은 초기상태의 상태값이 출력되어야 함
  
- Episode 진행
  - 초기 상태값을 환경에 입력하여 Action 값을 출력함
    - Action 값을 환경에 입력하여 다음 스텝의 상태값과 보상값 그리고 에피소드의 종료여부를 출력받음
    - Experience Replay에 현재 상태, 다음 상태, 현재 행위, 보상값을 넣어 저장함
    - 에피소드 종료 여부를 확인함(1 Step)
      - 종료하지 않았으면 다음 스텝의 상태값을 환경에 입력하여 Action을 출력함
      - 종료되었으면 다음으로 넘어감
  
- 학습 - 신경망 설계
  - Experience Replay에 저장되어있는 데이터 중에서 Batch Size 만큼 랜덤으로 데이터를 선택(Batch Size Sampling)
  - 인공신경망을 통해서 학습(입력으로는 State, 여러 단계를 거쳐서 Action이 나옴)
  - Hidden Layer와 Hidden Node에 따라 학습 효율, 성공 등 결과가 달라짐
  
- Target Q update
  - Target Q를 사용하는 이유는 Q Learing이 가지는 부트스트랩 문제 때문
  - 만약 Target Q를 Update 하지 않는다면 새로 개정된 문제집에 이전 버전의 답안지를 사용하는 것과 같음
  
- Frozen Lake 문제를 DQN으로 풀어보기  
  - Q-Network input
    - one-hot 방식으로 총 16개의 값
    - 존재할 수 있는 state의 수 16개
  
  - Q-Network output
    - 4개
    - 각 action에 대한 예측 Q 값
  
  - Q network 학습
    - 상태 s를 입력으로 넣으면 W(weight)와 연산하여 결과 Ws가 출력됨
    - Q table 때와 같이 수식을 이용하여 W를 학습
  
  - Q-Network cost function
    - 네트워크가 예측하는 Q값과 label(학습 기준)의 차이가 cost
    - 우리의 목표는 cost가 최소화하는 세타(weight)를 찾는 것
  
  - Q-Network Algorithm
    - Q Network의 weight를 랜덤하게 설정
    - for episode
      - 처음의 상태 s1을 가져오고 전처리 과정을 거침  
      - while not done
        - E-greedy 방법을 통해 action 결정
        - action을 취하고 나온 결과들을 저장
        - 나온 결과들로 Q-network 학습
        - 상태 업데이트
  
  - 결과(DQN을 사용한 Frozen Lake 문제 풀기)
    - Q-network의 성능이 Q-table보다 더 낮음
    - Q-table은 경우의 수가 많지 않기 때문에 전부 가봐서 확률이 높음
  
  - Q-network vs Q-table
    - DQN이 더 좋은 분야가 있음
      - 선택할 수 있는 Action의 종류가 많은 경우
      - State 상태가 매우 많은 경우