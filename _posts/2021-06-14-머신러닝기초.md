---
title: "Deep Learning"
excerpt: ""

categories:
  - 머신러닝
tags:
  - 머신러닝
  - ANN
---
# Deep Learning
### Artificial Neural Network(ANN)
- AI & ML & DL
  - AL(Artificial Intelligence)
    - 인간의 학습능력과 추론능력, 지각능력, 자연언어의 이해 능력 등을 컴퓨터 프로그램으로 실현한 기술
    - 이런 능력들중 컴퓨터 프로그래밍을 통해서 구현한다면 인공지능이라고 할 수 있으므로 매우 넓은 분야
  
  - Machine Learning(ML)
    - 인공지능의 연구 분야 중 하나로, 인간의 학습 능력과 같은 기능을 컴퓨터에서 실현하고자 하는 기술 및 기법
    - 학습이라는 개념이 중요
    - 사람이 규칙을 만드는 방식인 룰베이스가 아니라 데이터 기반으로 알고리즘이 스스로 학습한다는 특징
  
  - Deep Learning(DL)
    - 다층 구조 형태의 신경망을 기반으로 하는 머신러닝의 한 분야로 다양한 데이터로부터 높은 수준의 추상화 모델을 구축하고자 하는 기법
    - Deep Learning으로 ML 분야에 큰 발전이 있었으며 DL이 가장 각광받고 있는 기술

- Machine Learning의 학습 방법
  - Supervised Learning(지도 학습)
    - 정답이 있는 데이터를 이용하여 학습을 수행
    - Classification (사진을 보고 개인지 고양이인지 분류하는 문제)
    - Regression (데이터를 기반으로 수학적 모델을 찾는 문제)  
  - Unsupervised Learning(비지도 학습)
    - 정답을 모르고 데이터의 특징만을 이용하여 학습 수행
    - Clustering (데이터를 군집화)
    - Semi-supervised learning (Supervised Learning과 Unsupervised Learning을 혼합)  
  - Reinforcement Learning(강화학습)
    - 보상을 통해 학습을 수행
    - 특정 대상을 제어하면서 보상을 많이받고, 패널티는 피하는 방향으로 행동을 선택하도록 학습
    - 행동에 대한 명확한 보상을 설정할 수 있는 것에 적용하는 것이 좋음
    - Game에 적용 (AlphaGo, AlphaStar)

- Deep Learning
  - 2012년도 이전까지 크게 각광을 받는 기술이 아니였음
  - 이 시기까지는 딥러닝 이외의 머신러닝 기법들이 더 큰 주목을 받아왔었음
  - 세가지 발전을 통해서 Deep Learning이 발전하게 됨
  1. Algorithm
     - 딥러닝과 관련된 다양한 네트워크 구조, 학습 방법 등의 알고리즘이 개발되면서 딥러닝의 성능이 향상  
  2. GPU
     - 많은 행렬 연산이 필요한 딥러닝은 CPU 보다 GPU를 사용했을 때 효과가 좋음  
     - 게임 그래픽 발전에 따라서 GPU가 발전하게 되었고 이에 따라서 딥러닝의 연산속도가 대폭 향상  
  3. Big Data
     - 컴퓨터 하드웨어 및 인터넷 발전에 따라 많은 데이터를 저장 및 이용할 수 있도록 빅데이터를 구축하는 것이 가능
     - 딥러닝의 경우 많은 데이터를 통해 학습을 해야 좋은 성능을 얻을 수 있기 때문에 빅데이터의 구축이 딥러닝의 성능 향상에 기여
  
  - 딥러닝 발전의 시초는 이미지 분류 대회인 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)
  - 2012년부터 DL 기반 알고리즘이 CNN가 적용되면서 기존 기법에 대해 큰 성능 향상이 이뤄졌으며 그 이후로도 DL 기반 기법들이 쭉 대회를 우승함
  - 초기에는 간단한 DL 기술을 통해서 손글씨 인식, 표지판 인식, 텍스트 생성 등의 간단한 작업 수행
  - 이후 DL 기법 기반들은 다양한 분야에서 뛰어난 성능을 보이기 시작하며 굉장히 빠르게 발전
  - 2016년에는 세계적인 인공지능 회사 DeepMind에서 개발한 DL과 RL 기반 바둑 알고리즘 AlphaGo가 세계 최고 바둑 기사 이세돌과의 대결에서 4대 1로 승리하며 세계를 놀라게 함
  - 2019년에도 DeepMind에서 개발한 AlphaStar 알고리즘이 바둑보다 훨씬 환경이 복잡한 STAR CRAFT2에서 프로게이머를 상대로 10대 0의 승리를 거둠
  - DL 기반 알고리즘은 이미지 분야에서도 좋은 성능을 나타냄
  - DL 기반 알고리즘을 통해서 데이터를 생성하는 기술도 존재
  - 문장을 입력하면 문장에 해당하는 다양한 이미지를 생성해주는 기술도 존재
  - 위의 DL 기반에서 기본이 되는 알고리즘이 인공신경망(ANN)
  
- ANN(Artificial Nerual Network)
  - ANN
    - 사람의 뇌와 유사한 방법으로 정보를 처리하는 알고리즘
    - 인간의 두뇌는 뉴런(Neuron)이라는 기본 단위의 집합체로 구성
    - 뉴런이 굉장히 복잡한 구조로 얽혀서 사람의 뇌의 연산작용 수행
    - 인공신경망(ANN)은 Perceptron이라는 뉴런을 모방한 기본 단위의 집합으로 구성
  
  - Perceptron의 연산 과정
    - x1, x2, x3는 Perceptron의 입력
    - x1, x2, x3를 각각 가중치인 w1, w2, w3와 곱해주기
    - x1w1, x2w2, x3w3를 모두 더해주고 편향 b를 더해줌(이를 줄여서 h로 표현)
    - h를 비선형 함수 f()의 입력으로 사용한 output이 Perceptron의 최종 출력
    
  - 비선형 함수 f()
    - 시그모이드
    - 탄젠트하이퍼볼릭
    - 렐루(ReLU)
      - 입력의 크기가 0보다 작은 경우에는 0, 입력의 크기가 0보다 크거나 같은 경우에는 입력의 값과 같은 값 출력  
    - 리키렐루
    - 엘루

  - 인공신경망의 층
    - 입력층(Input Layer)
      - 인공신경망에 처음 입력이 들어오는 층  
    - 은닉층(Hidden Layer)
      - 많은 Perceptron으로 이루어진 인공신경망의 층
      - 각 입력들은 은닉층에 첫번째 층에 있는 각 Perceptron들의 입력으로 들어감
      - 각 Perceptron에서 연산을 수행하고 그 출력들은 은닉층의 두번째 층에 있는 각 Percetron들의 입력으로 들어감
    - 출력층(Output Layer)
      - 은닉층의 연산을 반복해서 최종적으로 출력하는 층
    - Forward Propagation: 위와 같이 입력층 > 은닉층 > 출력층을 거치는 과정
    - Back Propagation: 결과를 이미 알고 있는 Supervisor Learning의 경우 출력값과 실제 정답값을 비교하고 차이가 줄어들도록 네트워크 은닉층의 각 Perceptron 내부의 가중치와 편향을 학습하는 과정
    - FP와 BP를 반복해서 네트워크가 정답과 유사한 출력을 도출하도록 학습을 수행함
  
  - Loss Function(손실 함수)
    - 손실 함수를 최소화하도록 weight와 bias를 학습하는 것이 인공신경망 학습의 목표
    - ex) Mean Squared Error, Huber Loss, Cross Entropy 등
  
  - Gradient descent
    - Error를 최소로 하는 최적의 weight 학습을 위해 최적화 기법인 gradient descent 이용
    - Mini-batch Gradient Descent: 전체 데이터에서 임의로 일부 데이터를 추출하여 학습
      - 데이터를 임의로 섞어줌
      - Batch 단위만큼 한 묶음으로 묶어줌
      - Batch data를 하나씩 이용해서 네트워크 연산을 한 번 수행
      - 1 epoch: 전체 데이터에 대해 모두 학습을 수행하는 단위
      - 1 epoch으로 학습: 전체 데이터를 섞고, 이를 각 batch로 나눠준 후, 모든 batch에 대해 연산 및 학습을 수행
  
  - Fitting
    - Underfitting: 학습이 부족하여 제대로 된 예측을 수행하지 못함
    - Good fitting: 적절하게 학습이 수행되어 좋은 예측 수행
    - Overfitting: 과도하게 학습이 수행되어 training set에 대해서는 좋은 예측(처음 보는 데이터에 대해 나쁜 성능)
      - Overfitting이 발생하는 경우 Training Set의 손실함수값은 줄어들지만 Validation set의 손실함수 값은 어느 지점부터 증가하기 시작
      - 학습되지 않은 데이터에 대해 성능이 감소함
    
  - Dataset
    - Training Set: 학습을 위해 사용하는 데이터셋(전체 데이터의 약 70~80%)
    - Testing Set: 학습이 완료된 후 성능을 테스트하기 위해 사용하는 데이터셋(전체 데이터의 약 10~20%)
    - Validation Set: 하이퍼 파라미터 튜닝과 오버피팅 여부를 확인하기 위해 사용(전체 데이터의 약 10~20%)

  - Dropout
    - Overfitting을 방지하는 대표적인 기법 중 하나
    - 특정 확률에 따라 뉴런을 0으로 만들어서 네트워크의 결과에 변화를 주는 기법
    - 처음보는 데이터에 대해서도 결과를 잘 나타내는 일반화된 데이터를 만들어줌
  
  - ANN의 문제점
    - Input의 일부만 변경되어도 다른 Input으로 인식
    - 모든 데이터에 대응하기 위해서는 많은 데이터가 필요
    - 학습에 대한 시간도 많이 필요하고 성능도 제한적
  
## Convolutional Neural Network(CNN)
- CNN을 활용한 다양한 예시
  - Image Classification
  - Image-to-Image Translation(새로운 이미지로 변환)
  - Generative Model(새로운 이미지 만들기)
  - Super Resolution(화질 향상)
  - Object Detection(특정 물체 위치 파악 및 분류)
  - Segmentation(이미지의 픽셀 단위로 특정 물체 위치 파악 및 분류)

- Convolution
  - Filter를 이용하여 합성곱 연산을 수행하는 기법
  - 필터에 따라 이미지의 다른 특징을 추출
  - 1개의 필터는 1개의 Convolution output(Feature map)을 추출
  - 4개의 필터를 사용하면 4개의 feature maps를 생성하고 각각 필터마다 다른 특징의 결과를 도출
  
- Convolution 관련 용어
  - Channel: 이미지의 경우 3차원 데이터(높이 x 폭 x Channel 수)
    - 흑백 이미지의 경우: 1 채널
    - 칼러 이미지의 경우: 3 채널(RGB)
  
  - Kernel size
    - convolution을 하는 필터의 크기
    - 특징을 추출할 영역의 크기를 결정
  
  - Stride
    - 필터가 한번에 이동하는 정도를 나타내는 파라미터
    - Stride가 1이면 Filter를 한칸씩 이동시키며 연산 수행
    - 필터의 개수와 마찬가지로 스트라이드의 수도 output의 크기에 영향을 미침
  
  - Padding: 일반적으로 Convolution을 수행하는 경우 output의 크기가 줄어는데, Convolution의 output의 크기를 조절하기 위해서 바깥에 테두리를 쳐주는 기법

  - Polling: 특징 영역 내부의 정보를 압축하는 방식
    - Max Pooling: Window 내부의 값 중 가장 큰 값만을 선택
    - Average Pooling: Window 내부의 값을 평균
    - Convolution과 마찬가지로 filter size나 stride 조절 가능
    - 최근에는 Pooling을 하면 정보의 손실일 발생하기 때문에 Stride로 이미지의 크기를 줄이는 추세
  
- CNN의 구조
  1. 입력
     - 크기: 80x80x3  
  2. 합성곱
     - 필터 크기: 3x3x3
     - 필터 수: 4개
     - 스트라이드: 2x2
     - 패딩: same  
  3. 피처맵
     - 크기: 40x40x4  
  4. 합성곱
     - 필터 크기: 3x3x4
     - 필터 수: 8개
     - 스트라이드: 2x2
     - 패딩: same  
  5. 피처맵
     - 크기: 20x20x8
  6. 합성곱
     - 필터 크기: 3x3x8
     - 필터 수: 16개
     - 스트라이드: 2x2
     - 패딩: same  
  7. 피처맵
     - 크기: 10x10x16
  8. 벡터로 변환
     - 데이터 크기: 1600  
  9. 인공신경망 연산
     - 은닉층 크기: 512  
  10. 인공신경망 연산
     - 은닉층 크기: 256  
  11. 출력
     - 출력 크기: 4
  
### Recurrent Nerual Network(RNN)
- RNN 예시
  - NLP(Natural Language Processing)
    - 자연어처리
    - 최근에는 NLP를 통한 뉴스 생성까지 가능
    - 챗봇(일상적인 대화, 업무용 대화)  
  - Machine Translation(기계 번역)  
  - Image Captioning
    - 이미지를 보고 해당 이미지를 설명하는 문장을 생성해내는 기법(CNN + RNN)  
  - Sensor Data Analysis  
  
- RNN
  - CNN의 경우 이미지 데이터 처리에 좋은 성능을 지님
  - RNN의 경우 순차적 데이터(Sequential Data)의 처리에 주로 이용되는 알고리즘
    - 단어들이 모여서 만들어진 문장
    - 센서 데이터
  
- RNN 이론
  - 순서 또는 시간에 따라 데이터를 재귀적으로 전달(Sequential data 처리에 좋은 알고리즘)
  - 이전 Step의 output과 현재 step의 input을 함께 RNN의 입력으로 사용
  - 여러 Step의 정보를 종합하여 output을 도출할 수 있음

- RNN의 문제점
  - RNN에서 출력을 다음 step으로 전송할 때 weight를 곱하고 비선형 함수를 통과
  - 비선형 함수는 주로 하이퍼볼릭탄젠트 함수 사용
  - Backpropagation을 수행할 때 step이 진행될수록 gradient가 계속 줄어드는 문제(Vanishing Gradient Problem) 발생
  - Vanishing Gradient 문제를 해결하기 위해서 새로운 RNN 구조인 Gated Unit 개발
  
- LSTM(Long Short-Term Memory)
  - Cell State는 모든 데이터 위에 가로로 있으며 모든 Step에 걸쳐서 데이터를 전달, 모든 Step의 입력 정보를 반영하고 있는 중요한 데이터
  - 3개의 gate로 이루어진 구조(forget gate, input gate, output gate)
  - Forget gate: cell state에서 필요없는 데이터 제거
  - Input gate: 입력 정보 중 필요한 정보를 cell state에 새로운 정보 추가
  - Output gate: cell state를 이용하여 LSTM cell의 output을 출력
  
- Forget gate
  - 이전 step의 출력과 현재 step의 input을 concatenate하여 input으로 이용
  - 이어붙인 데이터에 가중치를 곱하고 편향을 더하는 선형 연산을 수행 후, 비선형 함수인 sigmoid 함수 통과
  - output의 범위는 0~1
  - output을 cell state에 곱하기(작은 값을 곱하는 경우 0에 가까워지면서 cell state에서 제외)
  
- Input gate
  - 이전 step의 출력과 현재 step의 input을 concatenate하여 input으로 이용
  - Tanh 함수를 이용하여 Input data 생성 -1~1
  - Sigmoid 함수를 이용하여 Input data 선별 0~1
  
- Cell State 데이터 중 필요없는 데이터를 지우고 새로운 데이터를 추가하는 과정
  - Forget gate의 출력을 Cell state에 곱하여 필요없는 데이터를 제외
  - input gate의 output을 cell state에 더하여 새로운 데이터를 추가
  - 새롭게 구한 cell state는 다시 다음 step의 LSTM cell로 전달
  
- Output gate
  - 새롭게 구한 cell state를 tanh 함수에 통과시켜 output data를 생성
  - 이전 step의 output과 현재 step의 input을 concatenate하고 새로운 입력을 sigmoid 통과시킨 후 output data에 곱하여 데이터를 선별
  - 최종적으로 구해진 output은 다시 다음 step의 LSTM cell에 전달
  
- RNN의 구조
  - 재귀적인 형태에 따라 다양한 구조 가능
  - one to one
    - 입력이 RNN cell을 통과해 바로 출력으로 나가는 재귀적 기능이 없는 함수  
  - one to many
    - 하나의 입력이 3번의 재귀적인 과정을 거치면서 3개의 출력 도출  
  - many to one
    - 3개의 순차적인 입력이 3번의 재귀적인 과정을 거쳐 하나의 출력 도출
    - 이때 출력은 기존 3개의 입력에 대한 모든 정보를 종합하여 도출  
  - many to many(1)
    - 3개의 순차적인 입력이 5번인 재귀적인 과정을 거쳐 3개의 출력 도출  
  - many to many(2)
    - 3개의 순차적인 입력이 3번인 재귀적인 과정을 거쳐 3개의 출력 도출  
  
# 강화학습
### 강화학습
- 강화학습의 개요
  - 행동에 따른 보상(reward)을 통한 학습
  
- GridWorld
  - Q-Learning 설명을 위한 게임
  - Agent는 우리가 control할 게임의 주인공이고, 행동은 A1~4방향으로 움직일 수 있음
  - 초록색 칸으로 가면 +1, 빨간색 칸으로 가면 -1
  - 초록색 칸으로 가면 게임이 종료한 후, 게임 1판이 끝나면 랜덤하게 재배치
  - 이 게임에 존재하는 state 수는 16x15x14 = 3360개

- 강화학습 용어 및 개요
  - Agent
    - 게임의 주인공
    - 알고리즘이 컨트롤할 대상
    - 학습의 대상  
  - Environment
    - 게임의 환경
    - 게임 그 자체  
  - State(s)
    - 게임의 상태(Agent의 위치, 빨간색 칸의 위치, 초록색 칸의 위치)  
  - Action(a)
    - Agent가 취할 수 있는 행동  
  - Reward(r)
    - 게임의 보상  
  - Terminal
    - 게임의 종료 여부  
  
- 강화학습 요약
  - Agent는 현재의 state를 통해 action을 선택(Environment가 변화)
  - Agent는 environment에서 얻은 정보(State, Reward, Terminal)를 이용하여 학습
  - 변한 State에 따라서 위의 과정 반복

- Q-value
  - 강화학습은 보상을 많이받는 방향으로 action으로 취하도록 하기 때문에 어떤 action이 얼마나 좋은지 기준 필요
  - 좋은 action이라는 기준에서 바로 다음 reward만 생각하면 문제가 발생할 수 있음
  - 미래에 받을 reward까지 고려할 필요가 있음
  - 단순 reward가 아닌 다른 것을 기준으로 사용해야 하는데 그때 이용하는 것이 Q-Learning의 Q값
  - Q값은 하나의 State와 Action에 대해서 존재하는 값
  - 특정 state에서 각 action은 미래 reward의 합

- Q-table
  - 하나의 Q 값은 하나의 State와 Action에 대해서 존재
  - 현재 State에서 Q값은 4개의 Action(a1~4)에 대해서 Q1~4까지 존재
  - Max Q를 가지는 action을 선택
  - 하지만 State는 하나만 존재하지 않기 때문에 이런 경우의 모든 State와 Action에 대한 Q값을 나타내주는 것이 Q-table
  
### Q-Learning
- 위 내용 정리
  - 하나의 State가 있으면 그 State를 Q-table에서 찾기
  - Q-table에서 해당 state에 대한 action 중 가장 큰 Q 값을 가진 Action 선택
  - 다음 State, reward, terminal에 대한 정보를 얻게 됨
  - 이 정보들이 모이면 해당 state와 action에 대한 update할 정보들이 모두 모임
  - 이 정보들로 Q-table 중 현재 state와 action의 값을 update
  - Next State를 현재 state로 하여 Q-table에서 찾고, 가장 큰 action을 가진 Q 값을 선택하고, update에 필요한 정보를 얻는 과정 반복
  
- Update 과정
  - 머신러닝의 학습 종류
    - Supervised Learning
      - 정답을 알고 있는 상태
      - 우리의 예측과 정답을 비교해서 예측을 정답에 가깝게 네트워크를 학습  
    - Unsupervised Learning
    - Reinforce Learning
      - Q-Learning에서 Supervised Learning 처럼 정답의 역할을 하는 것이 Target 값
  
  - Update
    - Q-value는 하나의 state와 action에 대해서 존재하는 값
    - 어떤 state에서 어떤 action을 취했을 때, 앞으로 받을 reward의 합의 예측값
    - 우리가 기존에 알고있는 정보: State, Action, Next state, Reward, Terminal
    - 앞으로 받을 reward의 합의 예측값 = 현재 state의 Reward, 다음 state의 Q-value
    - 현재 state에서 action을 취하면 reward를 얻게 됌
    - reward는 action을 통해서 얻은 실제 보상
    - 다음 state를 알고있으면 다음 state의 Q값들을 state를 통해서 알 수 있음
    - 다음 state의 Q 값은 다음 state으로부터 받을 것으로 기대되는 reward의 합
    - 현재 받은 보상과 다음 state부터 앞으로 받을 것으로 기대되는 보상을 더하면 현재 state에서 취한 action을 통해 받을 것으로 기대되는 reward의 총합을 구할 수 있음
    
  - target 값
    - 우리의 목표는 앞으로 받을 reward의 합이 최대가 될것이라고 생각되는 action을 취하도록 agent를 학습하는 것
    - 앞으로 우리가 받을 reward의 합은 reward의 다음 state의 Q 값을 더하기
    - 앞으로 받을 reward의 최대 합은 reward의 다음 state의 Q 값중 가장 큰 값을 더하기
    - 앞으로 받을 reward의 최대 합 즉, Target 값은 현재 State의 Reward + 다음 state의 Max Q
    
  - Gamma 값
    - Target 값을 구할 때에는 Gamma 값도 포함
    - 다음 state의 max Q값을 곱해주는 0~1 사이의 값
    - reward는 현재 state의 action을 취함으로써 실제로 얻은 보상값
    - 다음 state의 Max Q는 다음 State로부터 받게되는 reward의 합 중 최대 값
    - 미래에 고려되는 값들을 0~1로 곱해준 다음에 얼마나 고려할지 비율을 정하는 역할
    - 다른 이름으로 Discount Vector
  
  - target을 이용하여 Q값을 어떻게 Update 하는지
    - 새로운 Q값은 기존 Q값의 target 값을 일정비율 더해줘서 이용(알파값만큼 이용)
    - 기존 Q 값에는 (1-alpha)를 곱해주고 Target 값에는 alpha를 곱해준 후 더해줘서 사용
    - alpha는 학습을 얼마의 비율로 수행할지 결정
    - 학습의 비율을 결정하기 때문에 Learning Rate라고 부름
    
  - 정리
    - Q값이 학습하고 싶은 목표값인 Target 값은 Q값의 정의대로 앞으로 받을 것이라 예측되는 reward의 최대값
    - 그 목표값인 Target 값은 3가지 요소가 필요함
      - action을 취해서 실제로 얻은 reward
      - 미래의 reward의 합을 얼마나 반영할지 비율을 결정하는 gammma
      - 미래의 reward의 합의 예측값 중 가장 큰 값인 max Q  
    - gamma에 max Q를 곱하고 reward를 더해줘서 state에서 action을 취함으로써 받을 것이라 기대되는 미래까지 고려했을 때 보상의 합을 구해주기
    - 새로운 Q값은 기존의 Q값과 위에서 방금 구한 Target 값을 alpha 비율을 통해서 섞어줌

- Q-Learning 학습 예시
  - Q-table에 state가 없는 경우는 해당 State의 Q-value들은 0으로 초기화한 후, random action
  - max Q만을 따라서 움직이면 같은 state와 action이 반복됨

- Epsilon Greedy
  - Q-table이 학습이 되지 않은 Q값은 믿을 수 없음
  - 그렇다면 학습이 충분히 이루어지지 않고 다양한 경험을 해보지 못한 게임 초반에는 어떻게 할까?
  - Random하게 action을 하게 하여 다양한 경험을 하게 하기
  - 학습 초반에는 랜덤하게 action 결정, 후반에는 학습된 action 선택
  - Epsilon: 랜덤하게 action을 선택할 확률
  - Epsilon이 0.8이면 80% 확률로 Random action
  - 0부터 1사이의 랜덤값을 생성하고 그 값이 0.8보다 작으면 random action, 0.8보다 크면 max Q action
  - 처음 Epsilon의 값을 1로 설정했다가 학습이 진행함에 따라서 점점 값을 줄여나감

- Q-learning 전체 과정

### Deep Q Network
- Q-Learning의 한계와 DQN의 개선점
  - Q-Learning은 Q-Table안에 있는 여러 state와 action들에 대한 Q 값들을 학습하는 것이 목표
  - 같은 grid world game라고 해도 grid가 커지면 Q-table이 매우 커지게 됨
  - Q-Learning의 경우 특정 state에서 Q-table에 기반하여 action을 선택하며 Q-table의 해당 state와 action에서 학습하는 과정 수행
  - Q-table을 만들 수 없다는 것은 Q-learning 불가
  - state가 매우 다양해서 Q-table을 만들기 힘든 경우 Q-table처럼 하나의 input에 대응하는 하나의 결과가 반드시 존재해야하는 table은 사용할 수 없음
  - 이런 경우 결과를 근사해서 도출해주는 함수 필요(뉴럴 네트워크를 이용하기)
  
- DQN 이론 설명
  - 게임의 화면자체를 NN의 input으로 사용
  - 각 action에 대한 Q값을 output으로 도출
  - 하나의 state에 대해 하나의 action이 무조건 쌍으로 짝이 지어지는 Q-table과는 다르게 NN은 input에 대한 적절한 output을 추정
  - input으로 게임화면 그 자체 이미지를 이용하기 때문에 Q-table 대신에 사용하는 네트워크는 이미지 분석에 타고난 성능을 보이는 cnn 사용
  - cnn은 일반적으로 convolution과 pooling 과정을 거쳐 이미지의 특징을 뽑아내고 이를 nn의 input으로 사용하는 구조
  - DQN에서는 Game의 이미지를 input으로 하며 output으로는 각 action의 Q 값을 사용
  
- DQN의 학습과정
  - CNN + Q-Learning
  - 특별한 규칙에 대한 학습없이 게임 화면만을 통해 학습
  - Input: 게임 화면, Output: 각 action에 대한 value
  - Epslion Greedy를 통한 random action을 취할 수 있고, Max Q를 취할 수 있음
  - 각 action의 Q값이 CNN의 output
  - CNN의 output이 action 수만큼 존재하게 되고 그중 가장 큰 값이 Max Q action
  - 해당 action에 따라서 행동을 취하고 다음 state, reward, terminal 정보를 얻게 됨
  
- DQN과 Q-learning의 차이
  - input은 좌표값이나 사람의 특정값이 아닌 게임의 이미지 자체를 input으로 사용
  
- Update
  - Q-learning의 학습과정
    - Q값의 목표값인 Target 값을 구하고 그 값을 이용하여 Q-table의 Q 값을 update
    - Q값은 우리가 어떤 state에서 어떤 action을 취했을 때, 앞으로 받을 것이라고 예측하는 reward의 합
    - terminal이 True인 경우, 게임이 끝났기 때문에 다음 state는 Q 값을 가지지 않고, target값은 reward와 동일 
    - terminal이 False인 경우, reward의 다음 state에 Q 값중 가장 큰 Q 값을 선택하고, gamma와 곱해준 후 더해줌

  - DQN의 학습과정
    - Q값의 목표값인 Target 값 존재
    - 현재 추정한 값이 Target 값과 비슷하게 하는 것이 학습의 목표
    - Target 값에 현재 추정한 Q값을 빼고 제곱한 값을 최소화하는 방향으로 네트워크 학습
    - Q 러닝과 같이 learning rate alpha를 이용해서 학습을 얼마나 빠르게 할지 결정
    - 목표값과 현재 state와 action에 대해 계산된 Q 값의 차이가 최소가 되도록 학습
    - 학습이 진행될수록 네트워크는 목표값과 가까운 Q-value를 구하게 됨

### Deep Q Network 확장 기법
- Frame Skipping & Stacking
  - Stacking
    - 예를 들어서 pong game의 한 이미지만 보고 게임의 현재 상태(속도, 방향의 정보)를 아는 것은 어려움
    - 연속적인 프레임을 시간 순서대로 보면 공의 방향과 속도를 알 수 있으므로 시간에 대한 연속적인 프레임을 모두 한번에 CNN의 Input으로 이용하기
    - pygame에서 DQN 알고리즘으로 넘겨주는 이미지는 80x80x3(RGB)로 넘겨줌
    - Grayscale Image(80x80x1)로 변환하고 이것을 연속한 4개의 프레임에 대한 이미지를 stack해서 Stacked Image(80x80x4)를 만들어줌
  
  - Frame Skipping
    - 일반적으로 게임은 짧은 시간 안에 여러 프레임이 진행(ex. 1초에 30~60프레임)
    - 가까이 있는 프레임들은 큰 차이가 없음 -> 학습이 잘 안됨
    - 몇 프레임을 건너뛰면서 frame stacking을 수행(시간에 다른 변화가 분명함)
    
  - Frame Stacking 논문 방식(4 frame skip, 3 frame stacking)
    - Agent는 매 4번째 프레임들을 stack하여 CNN의 input으로 이용
    - 최신 프레임 이후 Skip 되는 프레임동안 같은 action을 반복
    - skip되는 동안 state에서는 input이 만들어지지 않기 때문에 action을 결정할 수 없음
      - x4가 되기 전까지 S1(x1, x2, x3)을 통해 선택한 action 반복
      - x5가 되기 전까지 S2(x2, x3, x4)을 통해 선택한 action 반복
      - x6가 되기 전까지 S3(x3, x4, x5)을 통해 선택한 action 반복
  
  - Frame Stacking 본 강의
    - 논문의 방식은 4프레임 동안 동일한 방식을 수행
    - 테트리스나 뱀게임 같은 형태의 게임은 여러 프레임 동안 같은 action을 반복하면 안됨
    - 예를 들어 한번 오른쪽 버튼을 누르면 4번 오른쪽 버튼으로 가는 등의 문제점이 발생할 수 있기 때문에 매 스텝 액션을 선택해줄 필요가 있음
    - 매 프레임마다 action을 선택할 수 있도록 DQN 방식을 그대로 사용하지 않고 변화를 줌
    - Agent는 매 4번째 프레임들을 stack하여 CNN의 input으로 이용
    - 매 프레임마다 frame skipping과 stacking을 수행
      - s1 = (x1, x2, x3), s2 = (x4, x5, x6), s3 = (x7, x8, x9)
    - 이 방식을 이용하면 계속 연속적인 state에서 action을 결정할 수 있으므로 같은 action을 반복할 필요 없이 매 action을 선택할 수 있음
  
- Experience Replay
  - 미니배치 학습 방식 적용
  - 즉, 하나의 데이터가 아닌 여러 데이터를 하나의 배치로 이용하여 한번에 학습 수행
  - 여러 데이터를 어떤 방식으로 하나의 배치로 구성하는지 알아보기
  - 일반적으로 게임은 짧은 시간 안에 여러 프레임이 진행
  - 가까이 있는 프레임들은 큰 차이가 없으므로 학습이 잘 안됨
  - 멀리 있는 프레임들은 서로 많이 다르므로 학습에 성능이 높음
  - 하나의 큰 데이터셋 안에 데이터들을 저장해놨다가 그 데이터중 랜덤하게 일부를 뽑아서 하나의 배치를 구성하고 학습 수행
  - 매 step마다 현재 state, action, reward, terminal, 다음 state 5개의 정보들을 하나의 experience 단위로 설정하고 replay memory라고 부르는 데이터셋에 저장
  - 데이터셋의 experience들이 매 step 쌓이게 되고 이중 몇개를 임의로 골라내서 하나의 batch로 만들기
  - 그 dataset을 이용하여 mini batch dataset을 사용
  
- Target Network
  - target 값을 구하는 식에서 세타는 뉴럴네트워크의 변수들
  - 네트워크의 weight, bias 등
  - 세타: 일반 Network의 변수들
  - 마이너스가 붙은 세타: target Network의 변수들
  - 일반 Network
    - 파라미터는 세타로 표시
    - 매 step마다 update
    - action을 구하기 위한 Q값이나 loss 계산을 위한 Q값을 계산할 때 이용
  
  - Target Network
    - 파라미터는 세타-로 표시
    - 일반 네트워크와 완전히 동일한 구조
    - 매 C step마다 한번씩 일반 네트워크의 변수들을 복제
    - C값은 직접 설정 가능
    - target 값을 계산할 때 이용하는 Q 값을 구할 때 이용하는 네트워크
    - 이 네트워크를 이용해서 target 값을 구하면 무엇이 좋을까
    - target 값은 supervised learning의 정답 역할, 이 값이 매 step update가 되면 매번 바뀌게 되는데, 같은 state에 대한 target 값이 바뀐다는 의미
    - 목표값이 계속 바뀌면 불안정한 학습을 의미함
    - 이에 따라 target이 적어도 C step 동안은 움직이지 않도록 해서 훨씬 안정적으로 구현 가능
  
- DQN 기반 추가 논문
  - 개요
    - 기존 DQN의 단점은 보완한 많은 논문들이 많이 발표
    - DQN 기반의 알고리즘을 다양하게 보완한 7가지 논문의 내용을 하나로 합친 Rainbow DQN 알고리즘 발표(2017)
    - 7가지 논문
       - DQN, Double DQN, Dueling DQN, Prioritized Expericence Replay, C51, Multi-step, NoisyNet DQN  
    - 이번 강의에서는 Double DQN(타겟값 변경)과 Dueling DQN(네트워크 구조 변경)에 대해 알아볼 것
  
- Double DQN
  - 논문
    - Deep Reinforcement Learing with Double Q-Learning
    - 2015.12.08 발표
    - https://arxiv.org/abs/1509.06461
  
  - Q-Learning의 문제점
    - 기존 Q-Learning의 경우 특정 환경에서 Q 값에 대한 overestimation 문제가 발생
    - 학습 성능 저하의 원인
  
  - overestimation 문제
    - 실제 value보다 예측 value이 훨씬 더 크게 도출되는 것
  
  - overestimation 문제가 발생하는 이유
    - 기존 Q-Learning의 경우 Single estimator 사용하여 타겟값을 도출
    - 하나의 네트워크를 이용하여 행동을 선택하고 가치를 평가하는 것이 overestimation의 원인
    
  - Double Q-Learning
    - Double estimator를 통해 행동 선택과 가치 평가를 분리
    - 일반 네트워크를 통해 행동을 결정하고 타겟 네트워크를 통해 가치 도출
    - 위 과정을 통해 Q값에 대한 overestimation 문제 해결
    
  - Target 값 구하는 방법
    - 일반 네트워크에 다음 상태를 입력으로 하여 각 행동에 대한 Q값을 도출함
    - 그 중 가장 큰 Q 값을 가지는 행동을 선택
    - 타겟 네트워크의 다음 상태를 입력으로 하여 각 행동에 대한 Q 값 도출
    - 여기서 최대 Q값을 사용하는 것이 아니라 타겟 네트워크를 통해 구해 각 행동의 Q값 중 일반 네트워크를 통해 구한 행동의 Q값을 최종적인 가치로 사용
    - 일반 네트워크를 통해 행동, 타겟 네트워크를 통해 가치를 도출하는 방식으로 overestimation 완화
  
  - Double Q-Learning의 성능
    - 예측가치의 크기와 분포가 작은 것을 보아서 overestimation 문제를 잘 방지
  
- Dueling DQN
  - 논문
    - Dueling Network Architectures for Deep Reinforcement Learing
    - 2016.04.05 발표
    - https://arxiv.org/abs/1511.06581
    
  - 네트워크 구조
    - DQN 네트워크의 Fully Connected 단을 2개로 나눴다가 다시 합치는 구조를 사용
    - CNN을 통해 인코딩 된 벡터를 두 부분으로 나눠서 연산
    - 한 부분은 1개의 값 state value
    - 다른 부분은 행동의 수와 동일 action advantage
  
  - Q-value 계산
    - State value와 action advantage를 결합하여 최종 Q-value 도출
    - alpha: action advantage function(fully connected layer)의 변수
    - betha: state value function (fully connected layer)의 변수
  
  - Dueling DQN 전체과정
    - 사진 첨부
  
  - Dueling DQN의 특징
    - action의 수가 많아지는 경우 더 빠른 학습 수행
  
  - Dueling DQN의 성능
    - Baseline: DDQN
    - 다수의 환경에서 Dueling DQN이 더 좋은 성능
  
- 두가지를 모두 합쳐서 구현한 것이 DDDQN

# Deep Deterministic Policy Gradient(DDPG)
- DDPG 개요
  - DDPG 논문
    - 2016년: Continuous Control with Deep Reinforcement Learing  
  - Deep Deterministic Policy Gradient(DDPG)
    - 기존 DQN 알고리즘은 이산적인 행동환경에만 적용 가능 (ex. 상하좌우)
    - 연속적인 행동이 필요한 환경에서는 적용할 수 없음 (ex. 로봇 팔의 움직임, 로켓의 엔진 분출량 결정)
    - DQN은 이미지와 같은 고차원 상태에 성공적으로 학습하는 결과를 보여주는 보여주었지만 고차원 행동에 대해서는 제한적
  
  - DDPG
    - Actor-Critic 기반 강화학습 알고리즘
    - DPG(Deterministic Policy Gradeient) 알고리즘에 심층인공신경망 기법을 적용
    - 연속적인 값 중에서 한 가지 행동 값을 출력
    - 기존의 정책 기반 강화학습 알고리즘에서는 가능한 행동에 대한 선택지가 있었고, 정책은 가능한 행동들 중 취할 확률을 출력했음
    - 하지만, DDPG는 결정적임. 행동 선택지가 따로 존재하여 정책이 그 행동들에 대한 확률 분포를 출력하는 것이 아니라 연속적인 값 중에서 한가지행동 값을 결정하여 출력
    - 즉, 선택지를 만들지 않고 실수 범위에서 행동 값을 출력할 수 있음
  
  - 가치 기반 강화학습 & 정책 기반 강화학습
    - 가치 기반 강화학습(Value-Based RL)
      - 가치를 기반으로 의사 결정
      - 큐 함수를 학습하여 최적의 큐 함수를 얻고 이를 통해 의사 결정
      - 대표적인 알고리즘: DQN(Deep Q Network)  
    - 정책 기반 강화학습(Policy-based RL)
      - 정책을 기반으로 의사 결정
      - 정책을 바로 학습하여 최적 정책을 얻고 이를 통해 의사 결정
      - 대표적인 알고리즘: REINFORCE  
    - Actor-Critic
      - 가치 기반과 정책 기반 강화학습을 결합
      - 두 종류의 네트워크 사용
        - Policy Network(정책 네트워크)
          - 정책을 직접 학습 -> Policy gradient 사용
          - 학습한 정책을 이용하여 에이전트의 의사 결정  
        - Value Network(가치 네트워크)
          - DQN과 동일하게 가치를 학습
          - 상태와 행동에 대한 가치 도출
  
  - DDPG 네트워크
    - 상태를 입력받아 행동을 출력하는 Actor 모델
    - 상태와 행동을 입력받아 Q 함수값을 예측하는 Critic 모델
    - Q 함수 값을 계산하는 하나의 네트워크로 이루어져 있던 DQN과는 다르게 행동을 결정하기 위한 Actor 모델 추가
  
  - DDPG 기법
    - DQN과 같이 네트워크 학습 성능 향상 및 안정적인 학습을 위해 여러 가지 기법 사용
    - 경험 리플레이(Experience Replay)
    - 타겟 네트워크(Target Network) SoftTarget Update를 통해 매 step 업데이트
    - OU Noise를 사용한 탐험
  
  - 경험 리플레이(Experience Replay)
    - DQN에서 사용된 것과 동일한 기법 사용
    - 연속적인 결정 과정에서 발생하는 샘플 간의 상관 관계 문제를 해결하기 위해 Replay memory에서 sample 단위로 학습을 하여 학습 효과 증가
    
  - 타겟 네트워크(Target Network)
    - DQN과 같이 타겟 네트워크를 따로 설정하여 학습의 안정성 증가
    - DQN에서는 일정 스텝마다 일반 네트워크의 파라미터들을 타겟 네트워크로 그대로 복사해서 업데이트하는 하드 타겟 업데이트 사용
    - DDPG에서는 DQN과 다르게 매 스텝마다 소프트 타겟 업데으트를 통해 타겟 네트워크를 업데이트
    - 소프트 타겟 업데이트(Soft Target Update)
      - 네트워크를 그대로 복사하지 않고 지수이동평균과 같은 방법을 통해 업데이트
      - 업데이트 비율을 의미하는 타우 값 설정
      - 이 비율만큼 타겟 네트워크의 가중치를 업데이트
      - 각 네트워크의 수식
      - 타우는 타겟 네트워크의 파라미터를 업데이트할 때, 일반 네트워크의 파라미터를 얼마나 반영할지 결정하는 값
      - 예를 들어 타우가 0이면 타겟 네트워크의 파라미터는 변하지 않고, 반대로 타우가 1이면 타겟 네트워크는 일반 네트워크와 동일  
    - OU 노이즈(Ornstein Uhlenbeck Noise)
      - DQN에서는 epsilon greedy 기법에 따라 random 행동을 통해 탐험을 할 수 있었음
      - 그러나 연속적인 행동 환경에서는 행동의 수가 무한하게 많음
      - 따라서 이 경우 행동에 대한 선택지를 만드는 것이 불가능
      - 선택지를 만들지 않고, 실수 범위에서 행동을 선택하여 탐험할 수 있는 방법이 필요
      - 이를 위해 random하게 평균으로 회귀하면서 noise를 생성하는 OU 노이즈 기법 적용
      - OU 노이즈를 생성하는 OU 프로세스는 기본적으로 x 라는 값이 있고 이를 기반으로 생성한 변화량 dx를 더해 값을 업데이트하여 랜덤한 실수 값을 만들어감
      - 값을 업데이트하는 수식
      - xt는 이전에 생성되었던 변화량 dx의 영향을 이어받아 과거 noise들과 시간적으로 연관되어있다는 특징을 가짐
      - t step에서 dx를 계산하는 식 오른쪽
      - 세타가 크면 빠르게 평균으로 회기
      - 뮤가 0
      - 세타와 시그마 값을 바꿔가며 총 3개의 노이즈 변화 곡선을 그린 그래프
        - 세타가 같고 시그마 값이 다른 경우(파랑, 주황) 
          - 파란색 선이 변동성을 의미하는 시그마가 더 크기 때문에 주황색 선보다 노이즈의 범위가 넒음  
        - 시그마 값이 같고 세타가 다른 경우(파랑, 초록)
          - 파란색 선이 변동성을 의미하는 시그마가 같지만 평균으로 회기하려는 속도를 의미하는 세타값이 커서 더 빨리 평균으로 회귀하려는 경향을 가짐
  
  - DDPG의 학습
    - 크리틱 네트워크 업데이트
      - DQN과 같이 Q 함수 값에 대한 예측값과 타겟값의 차이를 줄이는 방향으로 업데이트
      - 타겟값의 계산식(DQN과 동일)
      - 손실함수의 경우 예측값과 타겟값의 차이의 제곱 평균인 MSE로 설정
      - MSE의 수식(해당 Loss 값을 최소화하는 방향으로 학습 진행)
  
    - 액터 네트워크 업데이트
      - 정책기반 강화학습에서 정책 파이를 업데이트하기 위해 목표함수 J 설정
      - 목표함수를 최대화하는 방향으로 정책을 업데이트
      - 여기서 설정한 목표함수는 정책 파이를 통해 의사결정을 하였을 때, 초기 상태의 가치함수이고 이는 한 에피소드의 전체 가치를 의미
      - 수식
      - 가능한 모든 행동들의 확률 분포를 출력하는 것이 아니라 한 가지 행동만 출력하는 정책은 뮤라고 표기(앞으로 정책은 뮤로 표기)
      - 이 식을 Q 함수를 이용하여 변형하여 표기한 수식
      - 한 에피소드 동안 거쳤던 상태에서 취한 행동에 대한 Q 함수 값의 평균(정책 뮤를 통해 의사결정을 행했을 때, 한 에피소드에 대한 전체 가치를 의미)
      
      - 목표함수를 최대화하는 방향 계산을 위한 파라미터 업데이트 과정
        - 그래디언트 구하는 식
        - 액터 네트워크의 파라미터 업데이트는 연쇄법칙, Chain role이라는 미분 성질을 이용
        - 연쇄 법칙에서 가장 앞의 식 Q 함수를 정책 네트워크의 파라미터에 대해 미분한 것은
        - Q 함수를 행동에 대해 미분한 것의 행동을 정책 네트워크의 파라미터로 미분을 곱한 것과 같음
        - 연쇄 법칙을 이용해서 목표 함수의 Gradient를 다시 표현한 것이 두번째 식
        - 하지만 현재 진행하는 강화학습 환경에서는 상태 방문 확률 료를 알기 어려운 상황이기 때문에 가장 아래의 식과 같이 총 스텝 N으로 나눠 근삿값을 구함
  
  - DDPG의 전체 과정
    - Agent가 환경과 상호작용을 함(가장 아래 왼쪽 부분)
    - 이 때 Agent의 행동을 결정하는 것은 가장 아래의 Actor 네트워크
    - Actor 네트워크는 상태를 입력으로 하고 이에 따른 행동을 출력을 하는 네트워크
    - 이 때 이 행동의 OU 노이즈를 추가하여 탐험 수행
    - 환경과 Agent의 상호작용을 통해서 경험 데이터가 생성
    - 이 경험 데이터는 Replay Memory에 저장이 되며 한 Sample마다 미니배치 데이터를 샘플링하여 학습 수행
    - 먼저 Critic 네트워크는 상태와 행동을 입력으로 하여 해당 상태와 행동에 대한 Q 함수값을 도출
    - 일반 네트워크의 경우 현재 상태와 행동을 입력으로 하며, 타겟 네트워크의 경우 다음 상태, 해당 다음 상태를 타겟 actor 네트워크의 입력으로 했을 때 도출된 행동을 입력으로 사용
    - 일반 Critic 네트워크를 통해 구한 예측 Q 함수 값과 Target Critic 네트워크를 통해 구한 타겟 Q 함수값을 통해 손실 함수 값을 계산
    - 이를 최소화 하는 방향으로 Critic 네트워크를 업데이트
    - 타겟 Critic 네트워크의 경우 매 스텝마다 일반 Critic 네트워크들의 파라미터들에 대해 Soft target update를 수행
    - Actor 네트워크의 경우 Critic 네트워크를 통해 구한 Q 값을 최대화하는 방향으로 Policy Gradient를 통해 학습을 수행
    - 타겟 Actor 네트워크 또한 매 스텝마다 일반 Actor 네트워크의 파라미터들에 대해 소프트 타겟 업데이트를 수행하여 업데이트